
# EEG-to-Image Generation Dataset

This repository contains code and data for generating images from brain signals (EEG) using deep learning models. The project uses EEG recordings from subjects viewing ImageNet images and trains neural networks to map these brain signals to CLIP embeddings, which are then used with Stable Diffusion to generate images.

## Overview

The project implements an end-to-end pipeline that:
1. **Extracts frequency-domain features** from EEG signals using differential entropy (DE) across 5 frequency bands (delta, theta, alpha, beta, gamma)
2. **Trains an MLP mapper** to convert EEG features to CLIP text embeddings
3. **Generates images** using Stable Diffusion from the predicted embeddings
4. **Evaluates results** by comparing generated images with original ImageNet images

## Dataset

The EEG-ImageNet dataset contains:
- **EEG recordings** from 16 subjects viewing ImageNet images (62 EEG channels, 400ms duration)
- **42 ImageNet categories** with images from 10 coarse and 32 fine-grained classes
- **Frequency domain features** (Differential Entropy) pre-computed for 5 frequency bands

### Dataset Structure
```
data/
├── EEG-ImageNet.pth              # PyTorch dataset file
├── de_feat/                      # Pre-computed differential entropy features
│   ├── {subject}_{granularity}_de.npy
├── imageNet_images/              # Original ImageNet images
│   ├── {synset_id}/
│   └── synset_map_*.txt
└── mode/                         # Montage and channel configuration
```

## Architecture

### Models

1. **EEGNet**: Deep learning model for EEG signal processing
   - 4-layer CNN with depthwise separable convolutions
   - BatchNorm and Dropout for regularization

2. **MLP Mapper**: Multi-layer perceptron to map EEG features to CLIP embeddings
   - Input: 310-dim differential entropy features
   - Output: 77 × 768 CLIP text embeddings
   - Architecture: 4-layer MLP with BatchNorm and GELU activation

### Pipeline

```
EEG Signals → Frequency Features → MLP Mapper → CLIP Embeddings → Stable Diffusion → Images
```

1. **Feature Extraction**: Compute differential entropy for 5 frequency bands (delta, theta, alpha, beta, gamma) using MNE-Python
2. **Training**: MLP learns to predict CLIP embeddings from EEG features using MSE loss
3. **Generation**: Stable Diffusion generates images from predicted embeddings
4. **Caption-based Generation**: Alternative approach using BLIP-generated captions

## Installation

### Requirements
```bash
torch
torchvision
numpy
PIL
mne-python
diffusers
transformers
tqdm
```

### Setup
```bash
# Clone the repository
git clone <repository-url>
cd EEG-ImageNet-Dataset

# Install dependencies (recommended: use Python 3.10)
pip install -r requirements.txt

# Or create a virtual environment
python -m venv venv310
source venv310/bin/activate  # On Windows: venv310\Scripts\activate
pip install torch torchvision numpy pillow mne diffusers transformers tqdm
```

## Usage

### 1. Generate CLIP Embeddings from Captions

First, generate captions and CLIP embeddings from ImageNet images:

```bash
python src/blip_clip.py \
    -d ../data/ \
    -g all \
    -m mlp_sd \
    -s 0 \
    -o ../output/
```

This creates:
- `output/caption.txt`: Image captions generated by BLIP
- `output/clip_embeddings.pth`: CLIP text embeddings

### 2. Train MLP Mapper

Train the model to map EEG features to CLIP embeddings:

```bash
python src/image_generation.py \
    -d ../data/ \
    -g all \
    -m mlp_sd \
    -b 80 \
    -s 0 \
    -o ../output/
```

Arguments:
- `-d`: Dataset directory path
- `-g`: Granularity (`coarse`, `fine0-fine4`, `all`)
- `-m`: Model name (`mlp_sd`)
- `-b`: Batch size
- `-s`: Subject ID (0-15)
- `-o`: Output directory

### 3. Generate Images from EEG Signals

Generate images from EEG signals using the trained model:

```bash
python src/gen_eval.py \
    -d ../data/ \
    -g all \
    -m mlp_sd \
    -b 40 \
    -s 0 \
    -o ../output/
```

### 4. Generate Images from Captions (Alternative)

Generate images directly from text captions using Stable Diffusion:

```bash
python src/gen_from_caption_with_comparison_all.py \
    -d ../data/ \
    -o ../output/
```

This creates comparison grids showing original vs generated images.

## Directory Structure

```
EEG-ImageNet-Dataset/
├── src/                          # Source code
│   ├── dataset.py               # Dataset loader
│   ├── de_feat_cal.py           # Differential entropy feature extraction
│   ├── image_generation.py      # Training script
│   ├── gen_eval.py              # Image generation evaluation
│   ├── blip_clip.py             # Caption and embedding generation
│   ├── gen_from_caption_with_comparison_all.py  # Caption-based generation
│   ├── object_classification.py # Classification evaluation
│   └── model/
│       ├── eegnet.py            # EEGNet architecture
│       ├── mlp_sd.py            # MLP mapper architecture
│       ├── mlp.py               # Standard MLP
│       └── rgnn.py              # RGNN model
├── scipt/                        # Shell scripts
│   ├── generation_train.sh      # Training script
│   ├── generation_eval.sh       # Evaluation script
│   └── classification.sh        # Classification script
├── data/                         # Dataset
│   ├── EEG-ImageNet.pth
│   ├── de_feat/
│   ├── imageNet_images/
│   └── mode/
└── output/                       # Results and outputs
    ├── clip_embeddings.pth
    ├── caption.txt
    ├── eegnet_s0_1x_22.pth
    ├── mlp_sd_s0_model.pth
    └── generated_from_captions/
```

## Key Features

- **Multi-Frequency Analysis**: Extracts DE features from 5 EEG frequency bands
- **Cross-Modal Mapping**: Learns to map temporal-spatial EEG patterns to semantic embeddings
- **Stable Diffusion Integration**: Uses state-of-the-art diffusion models for image generation
- **Subject-Specific Training**: Supports individual subject models (0-15)
- **Granularity Control**: Coarse and fine-grained category classification


## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Contact

For questions or issues, please open an issue on GitHub.

